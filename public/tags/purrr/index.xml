<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
  <title>purrr on %&gt;% dreams</title>
  <link>/tags/purrr/</link>
  <description>Recent content in purrr on %&gt;% dreams</description>
  <generator>Hugo -- gohugo.io</generator>
<language>en-us</language>
<lastBuildDate>Thu, 28 May 2020 00:00:00 +0000</lastBuildDate><atom:link href="/tags/purrr/index.xml" rel="self" type="application/rss+xml" />
<item>
  <title>What It Takes to Tidy Census Data</title>
  <link>/blog/tidying-census-data/</link>
  <pubDate>Thu, 28 May 2020 00:00:00 +0000</pubDate>
  
<guid>/blog/tidying-census-data/</guid>
  <description>


&lt;p&gt;The U.S. Census releases aggregate data on their &lt;a href=&#34;https://www.census.gov/data/tables/2020/demo/hhp2.html&#34;&gt;Household Pulse Survey&lt;/a&gt;. These data are super interesting and cover a range of important topics, particularly those related to the COVID-19 pandemic.&lt;/p&gt;
&lt;p&gt;First of all, let me clarify that I think that the work that the Census does is amazing and I am so glad that these data are available. But, when you download the data, you will see that it is a highly stylized Excel spreadsheet. There may be upsides for those who want to see the data quickly and easily. As an R user though, seeing all those merged cells, non-numerics numerics, and category names in rows makes me feel &lt;code&gt;emo::ji(&#39;unamused&#39;)&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://ivelasq.rbind.io/img/census_files/census_image.png&#34; /&gt;&lt;/p&gt;
&lt;p&gt;However, this is not terribly surprising (and with public data, somewhat expected). As stated in the &lt;a href=&#34;https://cran.r-project.org/web/packages/tidyr/vignettes/tidy-data.html&#34;&gt;tidy data&lt;/a&gt; paper:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;It is often said that 80% of data analysis is spent on the cleaning and preparing data.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Thankfully, we have the very powerful R and tidyverse available to address our data woes. Let’s go through the process of tidying these data with tidyverse packages to show how easily they can become Ready for analysis!&lt;/p&gt;
&lt;div id=&#34;loading-the-data&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Loading the Data&lt;/h2&gt;
&lt;p&gt;Per usual, we begin by loading our necessary libraries.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(tidyverse)
library(readxl)
library(httr)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;An option to download the Excel file and loading it in R. What if we want to load the data directly from the website? We can use {httr}! The following code ‘gets’ the file from the internet, writes it in a temporary file path, and loads it in an object called &lt;code&gt;path&lt;/code&gt;.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Many thanks to Liz Potamites for pointing out: if the below doesn’t work, it may be that the link is changed or broken. It should be Table 2 from the second week of the Household Pulse Survey, which as of July 21, 2020 is located &lt;a href=&#34;https://www.census.gov/data/tables/2020/demo/hhp/hhp2.html&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;GET(&amp;quot;https://www2.census.gov/programs-surveys/demo/tables/hhp/2020/wk2/educ2_week2.xlsx&amp;quot;, write_disk(path &amp;lt;- tempfile(fileext = &amp;quot;.xlsx&amp;quot;)))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Response [https://www2.census.gov/programs-surveys/demo/tables/hhp/2020/wk2/educ2_week2.xlsx]
##   Date: 2020-07-22 03:02
##   Status: 200
##   Content-Type: application/vnd.openxmlformats-officedocument.spreadsheetml.sheet
##   Size: 442 kB
## &amp;lt;ON DISK&amp;gt;  /var/folders/pj/nmg9b8_93dq4kwt8nt2d4cj40000gn/T//RtmpfjZc3y/file7f32106c6185.xlsx&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;cleaning-the-data&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Cleaning the Data&lt;/h2&gt;
&lt;p&gt;As mentioned in the figure above, each sheet comprises of a state’s data. It’d be good to have all of the data in one single data structure. One option is to try to force all of the sheets together at once in a data frame (which is a 2D structure). But we also saw that each sheet requires a lot of cleaning before it can be useful, and it may be difficult to clean if they’re all merged in one data frame. Therefore, let’s instead first read in all the data as a &lt;strong&gt;list&lt;/strong&gt; (which is a higher dimension structure), clean it up, and &lt;em&gt;then&lt;/em&gt; put it together in a data frame.&lt;/p&gt;
&lt;p&gt;However, I am not very good at thinking about things in list format and it’s a little harder to see what’s going on compared to looking at a data frame using &lt;code&gt;View()&lt;/code&gt;. Before I clean up a list, I usually work on a single cut of the list as a data frame to know what exactly I am going to do. Thankfully, all the sheets in this Excel sheet are formatted the same across states. This isn’t always the case! Because they are identically formatted, we know if our processing works on one sheet, it will work across all of them.&lt;/p&gt;
&lt;p&gt;Let’s look at a single sheet!&lt;/p&gt;
&lt;div id=&#34;single-sheet-cleaning&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Single Sheet Cleaning&lt;/h3&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;census_sheet1 &amp;lt;-
  read_excel(path, sheet = 1)

View(census_sheet1)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://ivelasq.rbind.io/img/census_files/census1.png&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Immediately, we see that the top lines are superfluous rows (the headers from the original dataset). We can use &lt;code&gt;skip&lt;/code&gt; in &lt;code&gt;read_excel()&lt;/code&gt; to not have them read in.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;census_sheet1 &amp;lt;-
  read_excel(path, sheet = 1, skip = 3)

# View(census_sheet1)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://ivelasq.rbind.io/img/census_files/census2.png&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Now that the unnecessary rows are gone, we see that the column names aren’t reading in super well because of the merged cells in the original sheet. In this case, we manually create a vector of the column names and replace the old ones with &lt;code&gt;set_names()&lt;/code&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;new_names &amp;lt;-
  c(&amp;quot;select_characteristics&amp;quot;, &amp;quot;total&amp;quot;, &amp;quot;using_online_resources&amp;quot;, &amp;quot;using_paper_materials_sent_home&amp;quot;, &amp;quot;where_classes_were_cancelled&amp;quot;, &amp;quot;where_classes_changed_in_another_way&amp;quot;, &amp;quot;where_no_change_to_classes&amp;quot;, &amp;quot;did_not_respond&amp;quot;)

census_example &amp;lt;-
  census_sheet1 %&amp;gt;% 
  set_names(new_names)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://ivelasq.rbind.io/img/census_files/census3.png&#34; /&gt;&lt;/p&gt;
&lt;p&gt;We still have some empty rows (and also a row at the very bottom which is a note in the original dataset). We can eliminate these rows using &lt;code&gt;slice()&lt;/code&gt;. Here, we’re saying to ‘slice’ rows 1 through 3 and 60.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;census_example &amp;lt;-
  census_example %&amp;gt;% 
  slice(-1:-3, -60:-61)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://ivelasq.rbind.io/img/census_files/census4.png&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Now to deal with the fact that the category names are embedded within the first column &lt;code&gt;select_characteristics&lt;/code&gt;. There may be other ways to do this, but again I manually create a vector with all the characteristic names that I want to get rid of and use &lt;code&gt;filter()&lt;/code&gt; to keep only the rows that do &lt;strong&gt;not&lt;/strong&gt; contain the items in the vector.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;filter_var &amp;lt;- 
  c(&amp;quot;Age&amp;quot;, &amp;quot;Sex&amp;quot;, &amp;quot;Hispanic origin and Race&amp;quot;, &amp;quot;Education&amp;quot;, &amp;quot;Marital status&amp;quot;, &amp;quot;Presence of children under 18 years old&amp;quot;, &amp;quot;Respondent or household member experienced loss of employment income&amp;quot;, &amp;quot;Mean weekly hours spent on…&amp;quot;, &amp;quot;Respondent currently employed&amp;quot;, &amp;quot;Food sufficiency for households prior to March 13, 2020&amp;quot;, &amp;quot;Household income&amp;quot;)

census_example &amp;lt;-
  census_example %&amp;gt;% 
  filter(!select_characteristics %in% filter_var) &lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://ivelasq.rbind.io/img/census_files/census5.png&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Even though we removed the characteristic names from the rows, they actually contain very useful information. Also, we run into an issue in which two of the characteristic categories had the same options (“yes” and “no”). If we don’t address this, we’ll forget which rows are for which characteristic. To fix this, we manually create a column with the characteristics for each of the response options and append it to the data.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;category_column &amp;lt;-
  c(&amp;quot;age&amp;quot;, &amp;quot;age&amp;quot;, &amp;quot;age&amp;quot;, &amp;quot;age&amp;quot;, &amp;quot;age&amp;quot;, &amp;quot;sex&amp;quot;, &amp;quot;sex&amp;quot;, &amp;quot;race&amp;quot;, &amp;quot;race&amp;quot;, &amp;quot;race&amp;quot;, &amp;quot;race&amp;quot;, &amp;quot;race&amp;quot;, &amp;quot;education&amp;quot;, &amp;quot;education&amp;quot;, &amp;quot;education&amp;quot;, &amp;quot;education&amp;quot;, &amp;quot;marital_status&amp;quot;, &amp;quot;marital_status&amp;quot;, &amp;quot;marital_status&amp;quot;, &amp;quot;marital_status&amp;quot;, &amp;quot;marital_status&amp;quot;, &amp;quot;children&amp;quot;, &amp;quot;children&amp;quot;, &amp;quot;loss_employment&amp;quot;, &amp;quot;loss_employment&amp;quot;, &amp;quot;loss_employment&amp;quot;, &amp;quot;hours_spent&amp;quot;, &amp;quot;hours_spent&amp;quot;, &amp;quot;employed&amp;quot;, &amp;quot;employed&amp;quot;, &amp;quot;employed&amp;quot;, &amp;quot;food_sufficiency&amp;quot;, &amp;quot;food_sufficiency&amp;quot;, &amp;quot;food_sufficiency&amp;quot;, &amp;quot;food_sufficiency&amp;quot;, &amp;quot;food_sufficiency&amp;quot;, &amp;quot;income&amp;quot;, &amp;quot;income&amp;quot;, &amp;quot;income&amp;quot;, &amp;quot;income&amp;quot;, &amp;quot;income&amp;quot;, &amp;quot;income&amp;quot;, &amp;quot;income&amp;quot;, &amp;quot;income&amp;quot;, &amp;quot;income&amp;quot;)

census_example &amp;lt;-
  census_example %&amp;gt;% 
  add_column(category_column)&lt;/code&gt;&lt;/pre&gt;
&lt;center&gt;
&lt;img src=&#34;https://ivelasq.rbind.io/img/census_files/census6.png&#34; /&gt;
&lt;/center&gt;
&lt;p&gt;Finally, you may have noticed that some of the rows did not read in as numbers but as characters.&lt;/p&gt;
&lt;center&gt;
&lt;img src=&#34;https://ivelasq.rbind.io/img/census_files/census7.png&#34; /&gt;
&lt;/center&gt;
&lt;p&gt;We can use &lt;code&gt;mutate_at()&lt;/code&gt; and specify which variables we want to be numeric.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;census_example &amp;lt;-
  census_example %&amp;gt;% 
  mutate_at(vars(total, using_online_resources:did_not_respond), list(~ as.numeric(.)))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Hooray - now we have a tidy dataset we could use for analysis! Which is great, but it’s only one sheet. How do we do this for the additional 66?&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;multi-sheet-cleaning&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Multi Sheet Cleaning&lt;/h3&gt;
&lt;p&gt;We will now download the data and store it in a list, where each sheet (which represents a state) is saved as a tibble within the list. To work across all the lists, we use the tidyverse package {purrr} and its handy functions.&lt;/p&gt;
&lt;p&gt;You may notice that the multi sheet cleaning looks a lot like the single sheet cleaning but everything is wrapped in the function &lt;code&gt;map()&lt;/code&gt;. That’s true! The wonderful thing about {purrr} being in the tidyverse is that it’s really easy to integrate with all the tidyverse functions.&lt;/p&gt;
&lt;p&gt;Reading the data into one list is slightly more complicated than reading in a single sheet. We begin with the file path from before and then use &lt;code&gt;excel_sheets()&lt;/code&gt; to create a vector of the sheet names. &lt;code&gt;set_names()&lt;/code&gt; ensures that we have a named list that contains the state names, which will be important later. If we don’t use &lt;code&gt;set_names()&lt;/code&gt;, then the tibbles have generic names instead of ‘US’, ‘AL’, etc. Then using &lt;code&gt;purrr::map()&lt;/code&gt;, we ask R to download each of the sheets of the dataset and store it together in a list (&lt;code&gt;map()&lt;/code&gt; always returns a list).&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;census_list &amp;lt;-
  path %&amp;gt;% 
  excel_sheets() %&amp;gt;% 
  set_names() %&amp;gt;% 
  map(~ read_excel(path = path, sheet = .x, skip = 3), .id = &amp;quot;Sheet&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;If you take a look at the list using &lt;code&gt;View(census_list)&lt;/code&gt;, you can see the data is stored as tibbles within the list. If you expand &lt;code&gt;US&lt;/code&gt;, you’ll see the same data as when we did the single sheet example. You can also see the same data if you run &lt;code&gt;census_list[[&#34;US&#34;]]&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://ivelasq.rbind.io/img/census_files/census_list.png&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Using the same thinking as we did with the single sheet example, let’s go through and clean up this list - without having to go into each individual tibble!&lt;/p&gt;
&lt;p&gt;First, we set the names within each list using &lt;code&gt;set_names()&lt;/code&gt;. We tell &lt;code&gt;map()&lt;/code&gt; the names of the columns by defining &lt;code&gt;nm&lt;/code&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;census_list &amp;lt;- 
  census_list %&amp;gt;% 
  map(., set_names, nm = new_names)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;For each tibble in the list (&lt;code&gt;.x&lt;/code&gt;), remove the rows 1 through 3 and 60.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;census_list &amp;lt;- 
  census_list %&amp;gt;% 
  map(~ slice(.x, -1:-3, -60:-61))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now for each tibble, filter out the rows in &lt;code&gt;select_characteristics&lt;/code&gt; that contain the items in &lt;code&gt;filter_var&lt;/code&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;census_list &amp;lt;- 
  census_list %&amp;gt;% 
  map(~ filter(.x, !select_characteristics %in% filter_var))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Like before, we want a new column that lets us know the category for each of the characteristic options.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;census_list &amp;lt;- 
  census_list %&amp;gt;% 
  map(~ add_column(.x, category_column))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;And like before, we want to make sure our numeric columns are actually numeric.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;census_list &amp;lt;- 
  census_list %&amp;gt;% 
  map(~ mutate_at(.x, vars(total, using_online_resources:did_not_respond), list(~ as.numeric(.))))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now that our tibbles are all clean and uniform, let’s make this a single, 2D data frame! Like I mentioned before, it’s important that our list is named with the state abbreviations. We can use &lt;code&gt;map_df()&lt;/code&gt; to create a data frame with an ID column called &lt;code&gt;state&lt;/code&gt; that stores each of the sheet names. With this column, we’ll easily know which column is for which state/geography.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;census_df &amp;lt;- 
  census_list %&amp;gt;% 
  map_df(~ as.data.frame(.x), .id = &amp;quot;state&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Congrats! We have successfully tidied a Census dataset!&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;using-the-data&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Using the Data&lt;/h2&gt;
&lt;p&gt;The purpose of all this work is to be able to use it easily in R and with the tidyverse specifically. Let’s use the plotting package {ggplot2} to make something!&lt;/p&gt;
&lt;p&gt;According to the Census website, we can calculate percentages by removing those that did not respond from the total for the denominator (let’s presume that NA in the column means that everybody responded). Let’s say we want to see the proportion of respondents in the U.S. who say their classes were cancelled by income level.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;census_us_income &amp;lt;-
  census_df %&amp;gt;% 
  filter(state == &amp;quot;US&amp;quot;, category_column == &amp;quot;income&amp;quot;) %&amp;gt;% 
  mutate(responses = case_when(!is.na(did_not_respond) ~ total - did_not_respond, 
                               is.na(did_not_respond) ~ total),# calculate denominator
         pct_cancelled = where_classes_were_cancelled / responses) # calculate percentage

census_us_income &amp;lt;- # setting factor levels so graph shows correct order
  census_us_income %&amp;gt;% 
  mutate(select_characteristics = factor(select_characteristics,
                                         levels = c(&amp;quot;Less than $25,000&amp;quot;, 
                                                    &amp;quot;$25,000 - $34,999&amp;quot;,
                                                    &amp;quot;$35,000 - $49,999&amp;quot;,
                                                    &amp;quot;$50,000 - $74,999&amp;quot;,
                                                    &amp;quot;$75,000 - $99,999&amp;quot;,
                                                    &amp;quot;$100,000 - $149,999&amp;quot;,
                                                    &amp;quot;$150,000 - $199,999&amp;quot;,
                                                    &amp;quot;$200,000 and above&amp;quot;)))

census_us_income %&amp;gt;% 
  filter(select_characteristics != &amp;quot;Did not report&amp;quot;) %&amp;gt;% 
  ggplot(aes(x = select_characteristics, y = pct_cancelled)) +
  geom_bar(stat = &amp;quot;identity&amp;quot;,
           fill = &amp;quot;#265B5F&amp;quot;) +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1)) +
  scale_y_continuous(labels = scales::percent) +
  labs(title = &amp;quot;Percent of Respondents Whose Children&amp;#39;s Classes Were Cancelled&amp;quot;,
       x = &amp;quot;Income&amp;quot;,
       y = &amp;quot;Percent with Classes Cancelled&amp;quot;,
       caption = &amp;quot;Source: U.S. Census&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/blog/tidying-census-data_files/figure-html/unnamed-chunk-17-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;From this graph, we can see that respondents from the lower income bands were more likely to say that classes were cancelled for their children due to COVID.&lt;/p&gt;
&lt;/div&gt;
</description>
  </item>
  
<item>
  <title>Taking A Peek into My Hiking Data</title>
  <link>/blog/average-hike/</link>
  <pubDate>Sat, 06 Apr 2019 00:00:00 +0000</pubDate>
  
<guid>/blog/average-hike/</guid>
  <description>


&lt;p&gt;I moved to Seattle at the end of 2016 and since then have done over 100 hikes (depending on your definition of ‘a hike’!). I must admit I’ve been abysmal at tracking any data regarding my hiking activity beyond a &lt;a href=&#34;https://docs.google.com/spreadsheets/d/1Y3NdGea6yVuoDS7ewUKmuKGZouzU62FHK-aY813TafA/edit?usp=sharing&#34;&gt;Google spreadsheet&lt;/a&gt;, despite the ubiquity of trail tracking apps that exist.&lt;/p&gt;
&lt;p&gt;Recently, I signed up on &lt;a href=&#34;https://www.alltrails.com/&#34;&gt;AllTrails&lt;/a&gt; to start collecting data on my hikes. The Pro service offers many wonderful features, including the ability to download GPX data on hikes. I was so excited by this that I decided to try to visualize the hikes I have done.&lt;/p&gt;
&lt;p&gt;I’m structuring this article a bit differently with the results/visualizations first, but for anybody dying to see the data cleaning process, please see the &lt;a href=&#34;#methodology&#34;&gt;Methodology&lt;/a&gt; or &lt;a href=&#34;#viz&#34;&gt;Visualizations&lt;/a&gt; sections below! (Interesting, I ran &lt;a href=&#34;https://twitter.com/ivelasq3/status/1121536896956428289&#34;&gt;a poll on Twitter&lt;/a&gt; in which I asked whether people embed code in the main text of their blog post or at the end. 91% embed in the main text [n = 85]! Still, I prefer having the code at the end).&lt;/p&gt;
&lt;div id=&#34;analysis&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Analysis&lt;/h1&gt;
&lt;div id=&#34;disclaimer&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Disclaimer&lt;/h2&gt;
&lt;p&gt;For data collection, I downloaded each trail’s GPX files from AllTrails. Because these data are proprietary, I will not be providing them. Some things to note:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Because these are data pulled from the website, they are not indicative of my actual hiking path (for example, Franklin Falls is a 2 mile hike in the summer, but in the winter is a 6 mile snowshoe).&lt;/li&gt;
&lt;li&gt;There are hikes that I did back-to-back that I’d consider one hike but the trails might be listed separately on the site. For example, Deception Pass is actually made up of three small loops.&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;the-hikes-are-wide-and-varied&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;The hikes are wide and varied&lt;/h2&gt;
&lt;p&gt;Being fortunate enough to live near multiple mountain ranges, the hikes I’ve been on come in all shapes and sizes.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://ivelasq.rbind.io/img/average-hike_files/joyplot.png&#34; /&gt;&lt;/p&gt;
&lt;p&gt;I calculated my ‘average hike’ - that is, the average elevation given the cumulative distance travelled.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://ivelasq.rbind.io/img/average-hike_files/avg_hike.png&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;aggregated-data-by-trail&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Aggregated Data by Trail&lt;/h2&gt;
&lt;p&gt;In the aggregate, there seems to be a correlation (r^2 = 0.48) between total distance and total elevation.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://ivelasq.rbind.io/img/average-hike_files/tot_dis_elev.png&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;there-exist-categories-of-hikes&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;There Exist Categories of Hikes&lt;/h2&gt;
&lt;p&gt;I ran a quick &lt;a href=&#34;https://uc-r.github.io/kmeans_clustering&#34;&gt;cluster analysis&lt;/a&gt; to see if I can categorize my hikes in any way. Code is in the &lt;a href=&#34;#methodology&#34;&gt;Methodology&lt;/a&gt; section. Four clusters seemed to be optimal. I have dubbed them:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Cluster 1: “Let’s Get This Over With” (steep &amp;amp; hard)&lt;/li&gt;
&lt;li&gt;Cluster 2: “Easy Peasy Lemon Squeezy” (short &amp;amp; flat)&lt;/li&gt;
&lt;li&gt;Cluster 3: “The Sweet Spot” (not too long, not too high)&lt;/li&gt;
&lt;li&gt;Cluster 4: “I Don’t Care About My Knees Anyway” (too long for my own good)&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img src=&#34;https://ivelasq.rbind.io/img/average-hike_files/cluster_analysis.png&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;i-dont-particularly-love-long-hikes&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;I don’t particularly love long hikes&lt;/h2&gt;
&lt;p&gt;My average hike is 6.4 miles - and most of them are concentrated around that distance. This makes sense as I usually day hike and need to get back at a reasonable time. My shortest hike was 1.18 miles and my longest was 17.85 (the Enchantments…). In these 90 hikes, I hiked around 576 miles.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://ivelasq.rbind.io/img/average-hike_files/dist_histogram.png&#34; /&gt; &lt;img src=&#34;https://ivelasq.rbind.io/img/average-hike_files/cum_dist.png&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;i-dont-dislike-high-elevation-hikes-though&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;I don’t dislike high elevation hikes though&lt;/h2&gt;
&lt;p&gt;Elevation on these hikes ranged from ~0 feet to 4580 feet gain. I averaged 1455.4 feet gain and have climbed 130,984 feet (~24 miles!).&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://ivelasq.rbind.io/img/average-hike_files/elev_histogram.png&#34; /&gt; &lt;img src=&#34;https://ivelasq.rbind.io/img/average-hike_files/cum_elev.png&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;methodology&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Methodology&lt;/h1&gt;
&lt;div id=&#34;choose-packages&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Choose Packages&lt;/h2&gt;
&lt;p&gt;It took a bit to decide which packages had the functions needed to run the spatial analyses. In the end, I decided on:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;plotKML&lt;/strong&gt;: A package containing functions to read GPX files.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;geosphere&lt;/strong&gt;: A package containing functions for geospatial calculations. I decided to use this for finding out distances between lon/lat.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;googleway&lt;/strong&gt;: A package allowing access to the Google Maps API. To run this, you need to obtain a Google Maps API key and load it to R by using &lt;code&gt;set_key()&lt;/code&gt;. I use this for elevation calculations but the API can also obtain distance between points.&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(tidyverse)
library(googleway)
library(plotKML)
library(geosphere)

googleway::set_key(API_KEY_HERE)&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;upload-data&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Upload Data&lt;/h2&gt;
&lt;p&gt;I downloaded each GPX file from AllTrails and saved them in a file in my project organization. Their file names were TRAILNAME.gpx.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Using &lt;code&gt;plotKML::readGPX()&lt;/code&gt; results in the files being loaded as lists.&lt;/li&gt;
&lt;li&gt;I used &lt;code&gt;purrr&lt;/code&gt; in conjunction with &lt;code&gt;plotKML()&lt;/code&gt; to handily read them in and add the file name to the list.&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# find gpx files
data_path &amp;lt;- 
  here::here(&amp;quot;data&amp;quot;, &amp;quot;raw&amp;quot;, &amp;quot;gpx_files&amp;quot;)

files &amp;lt;-
  dir(data_path, pattern = &amp;quot;*.gpx&amp;quot;, full.names = TRUE)

# get trail names
names &amp;lt;-
  dir(data_path, pattern = &amp;quot;*.gpx&amp;quot;, full.names = FALSE) %&amp;gt;% 
  str_extract(&amp;quot;.+?(?=.gpx)&amp;quot;)

# read all gpx files
gpx_dat &amp;lt;-
  map2(files,
       names,
       ~ readGPX(.x,
         metadata = TRUE,
         bounds = TRUE,
         waypoints = TRUE,
         tracks = TRUE,
         routes = TRUE) %&amp;gt;%
         list_modify(trail = .y)) # otherwise you can&amp;#39;t tell which entry is for which trail&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;calculate-elevation&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Calculate Elevation&lt;/h2&gt;
&lt;p&gt;We can use &lt;code&gt;googleway::google_elevation()&lt;/code&gt; to access the Google Elevation API and calculate elevation for every lon/lat pair from the GPX files. Unfortunately, the API accepts and returns only a few requests at a time (~200 rows for these files). We have over 51,000 rows of data. So, we can create groups for every 200 rows and use a loop to make a call for each&lt;/p&gt;
&lt;p&gt;This results in a list, so we can then create a tibble pulling out the data we want.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;lonlat_dat &amp;lt;-
  gpx_dat %&amp;gt;%
  map_df(., ~.x$&amp;quot;routes&amp;quot;[[1]], .id = &amp;quot;trail&amp;quot;) %&amp;gt;%
  select(trail, lon, lat) %&amp;gt;% 
  group_by(trail) %&amp;gt;% 
  ungroup() %&amp;gt;% 
  mutate(group_number = (1:nrow(.) %/% 200) + 1) # https://stackoverflow.com/questions/32078578/how-to-group-by-every-7-rows-and-aggregate-those-7-values-by-median

dat_lapply &amp;lt;- lapply(1:max(lonlat_dat$group_number), function(x) {
  Sys.sleep(3)
  
  lonlat_dat %&amp;gt;%
    filter(group_number == x) %&amp;gt;% # added a filter so you only pull a subset of the data.
    do(elev_dat =
         data.frame(
           google_elevation(
             df_locations = dplyr::select(., lon, lat),
             location_type = &amp;quot;individual&amp;quot;,
             simplify = TRUE)))
  })

dat_lapply_elev_dat &amp;lt;-
  dat_lapply %&amp;gt;%
  map(., ~ .x$&amp;quot;elev_dat&amp;quot;[[1]])

elev_df &amp;lt;-
  dat_lapply_elev_dat %&amp;gt;% {
    tibble(
      elevation = map(., ~ .x$&amp;quot;results.elevation&amp;quot;),
      lon = map(., ~ .x$&amp;quot;results.location&amp;quot;[[&amp;quot;lng&amp;quot;]]),
      lat = map(.,  ~ .x$&amp;quot;results.location&amp;quot;[[&amp;quot;lat&amp;quot;]])
    )
  } %&amp;gt;% 
  unnest(.id = &amp;quot;group_number&amp;quot;) %&amp;gt;% 
  select(group_number, elevation, lon, lat)&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;calculate-distance&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Calculate Distance&lt;/h2&gt;
&lt;p&gt;Now we have a list of trails, longitudes and latitudes along their paths, and the elevation for each of those points. Now we want to calculate the distance along the paths.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;We bring back &lt;code&gt;lonlat_dat&lt;/code&gt; so we know what trails with which each points are associated.&lt;/li&gt;
&lt;li&gt;To use calculate distance, we can use &lt;code&gt;distHaversine()&lt;/code&gt; with two sets of lon/lat. We create the second set of lon/lat by creating a new variable that takes the “next” value in a vector (so we’re calculating the distance between point A and point B, point B to point C, and so on).&lt;/li&gt;
&lt;li&gt;&lt;code&gt;cumsum()&lt;/code&gt; accumulates the distances between each set of lon/lat.&lt;/li&gt;
&lt;li&gt;Finally, we calculate the elevation gain for each hike.&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;hiking_dat &amp;lt;-
  plyr::join(elev_df, lonlat_dat, type = &amp;quot;left&amp;quot;, match = &amp;quot;first&amp;quot;) %&amp;gt;% 
  group_by(trail) %&amp;gt;% 
  mutate(elev_feet = elevation * 3.281, # to convert to feet
         lon2 = lead(lon, 1),
         lat2 = lead(lat, 1)) %&amp;gt;%
  ungroup() %&amp;gt;% 
  mutate(dist = distHaversine(hiking_dat[, 2:3], hiking_dat[, 7:8])/1609.344) %&amp;gt;% # to convert to miles
  group_by(trail) %&amp;gt;% 
  mutate(cumdist = cumsum(dist),
         elev_gain = elev_feet - first(elev_feet)) %&amp;gt;%
  ungroup()&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;create-additional-tables&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Create Additional Tables&lt;/h2&gt;
&lt;p&gt;For nerdy kicks, I also wanted to find out my ‘average’ hike - that is, the average distance, the average elevation, and the average elevation for each distance. I also wanted to see the total distance and elevation for each trail for which I pulled data.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;avg_elev &amp;lt;- # average elevation by distance
  hiking_dat %&amp;gt;% 
  group_by(round(cumdist, 1)) %&amp;gt;% 
  summarize(mean(elev_gain))

hiking_dat_by_trail &amp;lt;- # total gain/distance by trail
  hiking_dat %&amp;gt;% 
  select(trail, cumdist, elev_gain) %&amp;gt;% 
  group_by(trail) %&amp;gt;%
  summarize(tot_dist = max(cumdist, na.rm = T),
            tot_elev_gain = max(elev_gain)) %&amp;gt;% 
  mutate(tot_dist_scaled = scale(tot_dist), # for cluster analysis
         tot_elev_scaled = scale(tot_elev_gain))&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;viz&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Visualizations&lt;/h1&gt;
&lt;p&gt;Below is the code for the visualizations presented above.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(tidyverse)
library(viridis)
library(ggridges)
library(cluster)
library(factoextra)

# joy plot

ggplot() + 
  geom_density_ridges(data = na.omit(hiking_dat),
                      aes(x = cumdist,
                          y = trail,
                          group = trail),
                      fill = &amp;quot;#00204c&amp;quot;,
                      rel_min_height = 0.01
                      ) +
  theme_minimal() +
  theme(legend.position = &amp;quot;none&amp;quot;)

# average hike

ggplot() + 
  geom_ridgeline(data = hiking_dat,
                 aes(x = cumdist,
                     y = trail,
                     group = trail,
                     height = elev_gain),
                 color = &amp;quot;#c9b869&amp;quot;,
                 alpha = 0) +
  geom_line(data = avg_elev,
            aes(x = `round(cumdist, 1)`,
                y = `mean(elev_gain)`),
            color = &amp;quot;#00204c&amp;quot;,
            size = 2) +
  scale_x_continuous(name = &amp;quot;Cumulative Distance (miles)&amp;quot;) +
  scale_y_continuous(name = &amp;quot;Cumulative Elevation (ft)&amp;quot;, limits = c(0, 5000)) +
  theme_minimal() +
  theme(legend.position = &amp;quot;none&amp;quot;)

# aggregate data scatterplot

ggplot() + 
  geom_point(data = hiking_dat_by_trail,
             aes(x = tot_dist,
                 y = tot_elev_gain,
                 color = tot_elev_gain,
                 size = tot_dist)) +
  scale_x_continuous(name = &amp;quot;Total Distance (miles)&amp;quot;) +
  scale_y_continuous(name = &amp;quot;Total Elevation (ft)&amp;quot;) +
  scale_color_viridis(option = &amp;quot;cividis&amp;quot;) +
  theme_minimal() +
  theme(legend.position = &amp;quot;none&amp;quot;)

# cluster analysis

fviz_nbclust(hiking_dat_by_trail[, 4:5], kmeans, method = &amp;quot;wss&amp;quot;) # finding optimal number of clusters
k4 &amp;lt;- kmeans(hiking_dat_by_trail[, 4:5], centers = 4, nstart = 25) # calculating clusters

fviz_cluster(k4, data = hiking_dat_by_trail)  +
  scale_x_continuous(name = &amp;quot;Scaled Total Distance (miles)&amp;quot;) +
  scale_y_continuous(name = &amp;quot;Scaled Total Elevation (ft)&amp;quot;) +
  scale_color_viridis(option = &amp;quot;cividis&amp;quot;, discrete = T) +
  scale_fill_viridis(option = &amp;quot;cividis&amp;quot;, discrete = T) +
  theme_minimal()

#  cumulative distance barplot

hiking_dat_by_trail %&amp;gt;% 
  mutate(cumdist = cumsum(tot_dist)) %&amp;gt;% 
  ggplot(aes(x = trail,
             y = cumdist,
             fill = cumdist)) +
  geom_bar(stat = &amp;quot;identity&amp;quot;) +
  scale_fill_viridis(option = &amp;quot;cividis&amp;quot;) +
  theme_minimal() +
  theme(legend.position = &amp;quot;none&amp;quot;)

# distance histogram

hiking_dat_by_trail %&amp;gt;% 
  ggplot(aes(x = tot_dist)) +
  geom_histogram(fill = &amp;quot;#00204c&amp;quot;) +
  xlab(&amp;quot;Trail Total Distance (miles)&amp;quot;) +
  ylab(&amp;quot;Count&amp;quot;) +
  scale_fill_viridis(option = &amp;quot;cividis&amp;quot;) +
  theme_minimal() +
  theme(legend.position = &amp;quot;none&amp;quot;)

# cumulative elevation barplot

hiking_dat_by_trail %&amp;gt;% 
  mutate(cumelev = cumsum(tot_elev_gain)) %&amp;gt;% 
  ggplot(aes(x = trail,
             y = cumelev,
             fill = cumelev)) +
  geom_bar(stat = &amp;quot;identity&amp;quot;) +
  scale_fill_viridis(option = &amp;quot;cividis&amp;quot;) +
  theme_minimal() +
  theme(legend.position = &amp;quot;none&amp;quot;)

# elevation histogram

hiking_dat_by_trail %&amp;gt;% 
  ggplot(aes(x = tot_elev_gain)) +
  geom_histogram(fill = &amp;quot;#00204c&amp;quot;) +
  xlab(&amp;quot;Trail Total Elevation (ft)&amp;quot;) +
  ylab(&amp;quot;Count&amp;quot;) +
  scale_fill_viridis(option = &amp;quot;cividis&amp;quot;) +
  theme_minimal() +
  theme(legend.position = &amp;quot;none&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
</description>
  </item>
  
<item>
  <title>Transforming PDF&#39;s into Useful Tables</title>
  <link>/blog/snap-expenditures/</link>
  <pubDate>Sun, 16 Dec 2018 00:00:00 +0000</pubDate>
  
<guid>/blog/snap-expenditures/</guid>
  <description>


&lt;p&gt;Way back in 2016, the USDA released a &lt;a href=&#34;https://www.fns.usda.gov/snap/foods-typically-purchased-supplemental-nutrition-assistance-program-snap-households&#34;&gt;study&lt;/a&gt; entitled “Foods Typically Purchased by Supplemental Nutrition Assistance Program (SNAP) Households,” including a summary, final report, and appendices. Per the USDA’s description:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;This study uses calendar year 2011 point-of-sale transaction data from a leading grocery retailer to examine the food choices of SNAP and non-SNAP households.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;At the time though, I was most interested in looking at the &lt;a href=&#34;https://fns-prod.azureedge.net/sites/default/files/ops/SNAPFoodsTypicallyPurchased-Appendices.pdf&#34;&gt;appendices data&lt;/a&gt; - &lt;em&gt;263&lt;/em&gt; pages full of tables detailing the commodities and categories of food bought by both families served and not served by SNAP. Unfortunately, these wonderful data are in PDF format, with ‘fancy’ Excel formatting (merged cells, unnecessary column names), where the formatting varies depending on which appendix you are looking at.&lt;/p&gt;
&lt;p&gt;I &lt;a href=&#34;mailto:SNAPHQ-WEB@fns.usda.gov&#34;&gt;emailed&lt;/a&gt; SNAP HQ to ask if they had the raw data available in CSV’s and was told simply:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Thank you for your inquiry. Unfortunately we do not have the data tables in a CSV file.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;At the time, my R skills were pretty rudimentary and I couldn’t figure out how to easily and efficiently pull the data into usable tables. Two years later and with a little more experience with R and scraping and cleaning ugly files, I decided to try again using the wonderful &lt;code&gt;tidyverse&lt;/code&gt; and &lt;code&gt;tabulizer&lt;/code&gt; package.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(tidyverse)
library(tabulizer)
library(broom)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Use &lt;code&gt;tabulizer::extract_tables()&lt;/code&gt; to extract the data from the &lt;a href=&#34;https://fns-prod.azureedge.net/sites/default/files/ops/SNAPFoodsTypicallyPurchased-Appendices.pdf&#34;&gt;appendices PDF&lt;/a&gt;. Once you extract the tables, you are left with a list (slightly more manageable than the original PDFs).&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;snap_pdf &amp;lt;-
  extract_tables(&amp;quot;https://fns-prod.azureedge.net/sites/default/files/ops/SNAPFoodsTypicallyPurchased-Appendices.pdf&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Using the &lt;code&gt;purrr&lt;/code&gt; package, create a data frame from the lists while simultaneously removing the unnecessary rows.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;snap_df &amp;lt;- 
  snap_pdf %&amp;gt;%
  map(as_tibble) %&amp;gt;%
  map_df(~ slice(., -2)) # slicing because of the unnecessary rows
  
head(snap_df)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 6 x 9
##   V1        V2    V3                    V4    V5    V6    V7    V8    V9   
##   &amp;lt;chr&amp;gt;     &amp;lt;chr&amp;gt; &amp;lt;chr&amp;gt;                 &amp;lt;chr&amp;gt; &amp;lt;chr&amp;gt; &amp;lt;chr&amp;gt; &amp;lt;chr&amp;gt; &amp;lt;chr&amp;gt; &amp;lt;chr&amp;gt;
## 1 &amp;quot;&amp;quot;        &amp;quot;&amp;quot;    &amp;quot;SNAP Household Expe~ &amp;lt;NA&amp;gt;  &amp;lt;NA&amp;gt;  &amp;lt;NA&amp;gt;  &amp;lt;NA&amp;gt;  &amp;lt;NA&amp;gt;  &amp;lt;NA&amp;gt; 
## 2 Soft dri~ &amp;quot;&amp;quot;    1 $357.7 5.44% 2 $1,~ &amp;lt;NA&amp;gt;  &amp;lt;NA&amp;gt;  &amp;lt;NA&amp;gt;  &amp;lt;NA&amp;gt;  &amp;lt;NA&amp;gt;  &amp;lt;NA&amp;gt; 
## 3 Fluid mi~ &amp;quot;&amp;quot;    2 $253.7 3.85% 1 $1,~ &amp;lt;NA&amp;gt;  &amp;lt;NA&amp;gt;  &amp;lt;NA&amp;gt;  &amp;lt;NA&amp;gt;  &amp;lt;NA&amp;gt;  &amp;lt;NA&amp;gt; 
## 4 Beef:gri~ &amp;quot;&amp;quot;    3 $201.0 3.05% 6 $62~ &amp;lt;NA&amp;gt;  &amp;lt;NA&amp;gt;  &amp;lt;NA&amp;gt;  &amp;lt;NA&amp;gt;  &amp;lt;NA&amp;gt;  &amp;lt;NA&amp;gt; 
## 5 Bag snac~ &amp;quot;&amp;quot;    4 $199.3 3.03% 5 $79~ &amp;lt;NA&amp;gt;  &amp;lt;NA&amp;gt;  &amp;lt;NA&amp;gt;  &amp;lt;NA&amp;gt;  &amp;lt;NA&amp;gt;  &amp;lt;NA&amp;gt; 
## 6 Cheese    &amp;quot;&amp;quot;    5 $186.4 2.83% 3 $94~ &amp;lt;NA&amp;gt;  &amp;lt;NA&amp;gt;  &amp;lt;NA&amp;gt;  &amp;lt;NA&amp;gt;  &amp;lt;NA&amp;gt;  &amp;lt;NA&amp;gt;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Due to the original formatting of the PDFs, there’s a bunch of cleaning to be done to make the list into a usable table. Using &lt;code&gt;slice()&lt;/code&gt;, I choose only the rows from Appendix 1. I manually put in the number of entries (238) because I didn’t know how to calculate this using code. If you have ideas, let me know!&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;snap_appendix1 &amp;lt;-
  snap_df %&amp;gt;% 
  slice(1:238) # Appendix A has 238 rows&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now comes the fun part (yay data cleaning!). When looking at the data frame, the data for each commodity are all mushed together in two separate columns (V2 and V3), but only one column or the other. Then there are a bunch of empty columns (V4 through V9), probably created from the funky original formatting. The data all begin with numbers as well. First things first - put all the data in a single column. Then, remove all the empty rows in the newly created &lt;code&gt;col_dat&lt;/code&gt; column.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;snap_appendix1_cleaned &amp;lt;-
  snap_appendix1 %&amp;gt;%
  mutate(col_dat = case_when(grepl(&amp;quot;[0-9]&amp;quot;, V2) ~ V2, # create a column that contains all the data
                           grepl(&amp;quot;[0-9]&amp;quot;, V3) ~ V3,
                           TRUE ~ &amp;quot;&amp;quot;)) %&amp;gt;% 
  filter(col_dat != &amp;quot;&amp;quot;) # some rows are empty

head(snap_appendix1_cleaned)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 6 x 10
##   V1       V2    V3         V4    V5    V6    V7    V8    V9    col_dat    
##   &amp;lt;chr&amp;gt;    &amp;lt;chr&amp;gt; &amp;lt;chr&amp;gt;      &amp;lt;chr&amp;gt; &amp;lt;chr&amp;gt; &amp;lt;chr&amp;gt; &amp;lt;chr&amp;gt; &amp;lt;chr&amp;gt; &amp;lt;chr&amp;gt; &amp;lt;chr&amp;gt;      
## 1 Soft dr~ &amp;quot;&amp;quot;    1 $357.7 ~ &amp;lt;NA&amp;gt;  &amp;lt;NA&amp;gt;  &amp;lt;NA&amp;gt;  &amp;lt;NA&amp;gt;  &amp;lt;NA&amp;gt;  &amp;lt;NA&amp;gt;  1 $357.7 5~
## 2 Fluid m~ &amp;quot;&amp;quot;    2 $253.7 ~ &amp;lt;NA&amp;gt;  &amp;lt;NA&amp;gt;  &amp;lt;NA&amp;gt;  &amp;lt;NA&amp;gt;  &amp;lt;NA&amp;gt;  &amp;lt;NA&amp;gt;  2 $253.7 3~
## 3 Beef:gr~ &amp;quot;&amp;quot;    3 $201.0 ~ &amp;lt;NA&amp;gt;  &amp;lt;NA&amp;gt;  &amp;lt;NA&amp;gt;  &amp;lt;NA&amp;gt;  &amp;lt;NA&amp;gt;  &amp;lt;NA&amp;gt;  3 $201.0 3~
## 4 Bag sna~ &amp;quot;&amp;quot;    4 $199.3 ~ &amp;lt;NA&amp;gt;  &amp;lt;NA&amp;gt;  &amp;lt;NA&amp;gt;  &amp;lt;NA&amp;gt;  &amp;lt;NA&amp;gt;  &amp;lt;NA&amp;gt;  4 $199.3 3~
## 5 Cheese   &amp;quot;&amp;quot;    5 $186.4 ~ &amp;lt;NA&amp;gt;  &amp;lt;NA&amp;gt;  &amp;lt;NA&amp;gt;  &amp;lt;NA&amp;gt;  &amp;lt;NA&amp;gt;  &amp;lt;NA&amp;gt;  5 $186.4 2~
## 6 Baked b~ &amp;quot;&amp;quot;    6 $163.7 ~ &amp;lt;NA&amp;gt;  &amp;lt;NA&amp;gt;  &amp;lt;NA&amp;gt;  &amp;lt;NA&amp;gt;  &amp;lt;NA&amp;gt;  &amp;lt;NA&amp;gt;  6 $163.7 2~&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now the numeric data we want is wonderfully in a single column, so we can select the columns &lt;code&gt;V1&lt;/code&gt; and &lt;code&gt;col_dat&lt;/code&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;snap_appendix1_cleaned &amp;lt;-
  snap_appendix1_cleaned %&amp;gt;% 
  select(V1, col_dat)

head(snap_appendix1_cleaned)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 6 x 2
##   V1                  col_dat                        
##   &amp;lt;chr&amp;gt;               &amp;lt;chr&amp;gt;                          
## 1 Soft drinks         1 $357.7 5.44% 2 $1,263.3 4.01%
## 2 Fluid milk products 2 $253.7 3.85% 1 $1,270.3 4.03%
## 3 Beef:grinds         3 $201.0 3.05% 6 $621.1 1.97%  
## 4 Bag snacks          4 $199.3 3.03% 5 $793.9 2.52%  
## 5 Cheese              5 $186.4 2.83% 3 $948.9 3.01%  
## 6 Baked breads        6 $163.7 2.49% 4 $874.8 2.78%&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;All the numeric data is still mushed in column &lt;code&gt;col_dat&lt;/code&gt;, so using &lt;code&gt;tidyr::separate()&lt;/code&gt; we can split the values because they are all separated by spaces. Referencing the original PDF, we descriptively rename the columns (and rename the commodity column &lt;code&gt;V1&lt;/code&gt; as well). The numeric values have retained their original formatting, with dollar signs and commas and percentage signs, oh my! We can sub out those unnecessary characters and transform those columns into truly numeric values.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;snap_appendix1_cleaned &amp;lt;-
  snap_appendix1_cleaned %&amp;gt;%
  separate(col_dat, &amp;quot; &amp;quot;,
           into = c(&amp;quot;snap_rank&amp;quot;, &amp;quot;snap_dollars_in_millions&amp;quot;, &amp;quot;snap_pct_total_expenditures&amp;quot;, &amp;quot;nonsnap_rank&amp;quot;, &amp;quot;nonsnap_dollars_in_millions&amp;quot;, &amp;quot;nonsnap_pct_total_expenditures&amp;quot;)) %&amp;gt;%
  rename(commodity = V1) %&amp;gt;%
  mutate_at(vars(snap_rank:nonsnap_pct_total_expenditures), list(~ as.numeric(gsub(&amp;quot;,|%|\\$&amp;quot;, &amp;quot;&amp;quot;, .))))

head(snap_appendix1_cleaned)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 6 x 7
##   commodity snap_rank snap_dollars_in~ snap_pct_total_~ nonsnap_rank
##   &amp;lt;chr&amp;gt;         &amp;lt;dbl&amp;gt;            &amp;lt;dbl&amp;gt;            &amp;lt;dbl&amp;gt;        &amp;lt;dbl&amp;gt;
## 1 Soft dri~         1             358.             5.44            2
## 2 Fluid mi~         2             254.             3.85            1
## 3 Beef:gri~         3             201              3.05            6
## 4 Bag snac~         4             199.             3.03            5
## 5 Cheese            5             186.             2.83            3
## 6 Baked br~         6             164.             2.49            4
## # ... with 2 more variables: nonsnap_dollars_in_millions &amp;lt;dbl&amp;gt;,
## #   nonsnap_pct_total_expenditures &amp;lt;dbl&amp;gt;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Last but not least, we convert all the columns with percentages into actual percentages by dividing by 100.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;snap_appendix1_cleaned &amp;lt;-
  snap_appendix1_cleaned %&amp;gt;%
  mutate_at(vars(contains(&amp;quot;pct&amp;quot;)), list(~ ./100))

head(snap_appendix1_cleaned)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 6 x 7
##   commodity snap_rank snap_dollars_in~ snap_pct_total_~ nonsnap_rank
##   &amp;lt;chr&amp;gt;         &amp;lt;dbl&amp;gt;            &amp;lt;dbl&amp;gt;            &amp;lt;dbl&amp;gt;        &amp;lt;dbl&amp;gt;
## 1 Soft dri~         1             358.           0.0544            2
## 2 Fluid mi~         2             254.           0.0385            1
## 3 Beef:gri~         3             201            0.0305            6
## 4 Bag snac~         4             199.           0.0303            5
## 5 Cheese            5             186.           0.0283            3
## 6 Baked br~         6             164.           0.0249            4
## # ... with 2 more variables: nonsnap_dollars_in_millions &amp;lt;dbl&amp;gt;,
## #   nonsnap_pct_total_expenditures &amp;lt;dbl&amp;gt;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Tada! Now we have a clean dataset from the original not-very-usable PDFs.&lt;/p&gt;
&lt;p&gt;Another goal is to at some point do a full analysis of what these data show. My hope is that now some of it is available, others can create and share amazing analysis with it. For the purposes of this blogpost, here are some quick ggplots comparing the rank of commodities between families served by SNAP and those who are not.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;snap_appendix1_cleaned %&amp;gt;% # quick correlation plot
 ggplot(aes(x = snap_rank, y = nonsnap_rank)) +
 geom_point()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/blog/snap-expenditures_files/figure-html/unnamed-chunk-9-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;snap_lm &amp;lt;- lm(snap_rank ~ nonsnap_rank, data = snap_appendix1_cleaned)
snap_res &amp;lt;- augment(snap_lm)

snap_res %&amp;gt;% # quick residual plot
  ggplot(aes(x = .fitted, y = .resid)) +
  geom_point()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/blog/snap-expenditures_files/figure-html/unnamed-chunk-9-2.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;I’d love to collaborate with others to finish up this project and find efficiencies. The repo with the code and final dataset is &lt;a href=&#34;https://github.com/ivelasq/snap&#34;&gt;here&lt;/a&gt;. More to come!&lt;/p&gt;
</description>
  </item>
  
<item>
  <title>Disaggregating Minneapolis Public Schools Data</title>
  <link>/blog/why-disaggregate-data/</link>
  <pubDate>Fri, 21 Sep 2018 00:00:00 +0000</pubDate>
  
<guid>/blog/why-disaggregate-data/</guid>
  <description>&lt;p&gt;Although this post heavily references education and student data, the principle of using distributions is paramount in any field that strives to close equity gaps.&lt;/p&gt;
&lt;div id=&#34;what-is-equity&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;What is Equity&lt;/h2&gt;
&lt;p&gt;Inequities are &lt;a href=&#34;http://www.yourdictionary.com/racial-inequality&#34;&gt;disparities in opportunity, resources, and treatment.&lt;/a&gt; Racial and socioeconomic inequities are those that are as a result of one’s race or socioeconomic status. The unequal distribution of opportunity, resource, and treatment can be due to many factors, including patterns in society. &lt;a href=&#34;https://www.allsides.com/dictionary/racial-inequity&#34;&gt;Examples include:&lt;/a&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Racial attitudes/bias that act subtly to undermine and exclude,&lt;/li&gt;
&lt;li&gt;Continued redlining in lending,&lt;/li&gt;
&lt;li&gt;Embedded biases in education (images, language, school discipline),&lt;/li&gt;
&lt;li&gt;Long term ramifications of poor health/healthcare.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Because the definition of equity varies from team to team, it is important to decide which inequities to identify and focus on.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;what-is-disaggregated-data&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;What Is Disaggregated Data&lt;/h2&gt;
&lt;p&gt;According to the &lt;a href=&#34;https://www.edglossary.org/disaggregated-data/&#34;&gt;Glossary of Education Reform&lt;/a&gt;, the formal definition of disaggregated data is:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Disaggregated data refers to numerical or non-numerical information that has been (1) collected from multiple sources and/or on multiple measures, variables, or individuals; (2) compiled into aggregate data—i.e., summaries of data—typically for the purposes of public reporting or statistical analysis; and then (3) broken down in component parts or smaller units of data.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Aggregate population numbers are broken down into smaller groupings that analysts can compare and contrast. These groupings depend on your team’s definition of equity, whether it be focused on race, socioeconomic status, race AND socioeconomic status, age, ethnicity, etc.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;why-use-disaggregated-data&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Why Use Disaggregated Data&lt;/h2&gt;
&lt;p&gt;Particularly in education, disaggregated data is essential in identifying where solutions are needed to solve inequities. Per NCES’ &lt;a href=&#34;https://nces.ed.gov/pubsearch/pubsinfo.asp?pubid=NFES2017017&#34;&gt;Forum Guide to Collecting and Using Disaggregated Data on Racial/Ethnic Subgroups&lt;/a&gt;:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Educators need both high-level data summaries as well as disaggregated data that accurately describe smaller groups of students they serve. Access to and analysis of more detailed data—that is, disaggregated data—can be a useful tool for improving educational outcomes for small groups of students who otherwise would not be distinguishable in the aggregated data used for federal reporting. Disaggregating student data into subpopulations can help schools and communities plan appropriate programs; decide which interventions to implement; target limited resources; and recognize trends in educational participation, outcomes, and achievement.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;/div&gt;
&lt;div id=&#34;minneapolis-public-schools-example&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Minneapolis Public Schools Example&lt;/h2&gt;
&lt;p&gt;Minneapolis Public Schools (MPS) reports their student demographics in a robust, complete way. Not only do they report the percentage of students in a subgroup, but they also include the number of students in each subgroup. This allows a deep look into their individual school demographics and gives us the opportunity to explore equity in their district.&lt;/p&gt;
&lt;div id=&#34;pulling-data&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Pulling Data&lt;/h3&gt;
&lt;p&gt;All code to pull MPS data is included in the code appendix. MPS has moved to publishing their school data in PDFs. Thankfully, the &lt;code&gt;tabulizer&lt;/code&gt; package exists! It easily and quickly pulled the data into lists which I then transformed to data frames and tidied up.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;first-glance-mps-district-demographics&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;First Glance: MPS District Demographics&lt;/h3&gt;
&lt;p&gt;Here is a barplot which shows the percentage of different subgroups in the school district. FRPL stands for Free/Reduced Price Lunch, often used as a proxy for poverty. Students from a household with an income up to 185 percent of the poverty threshold are eligible for free or reduced price lunch. (Sidenote: Definitions are very important in disaggregated data. FRPL is used because it’s ubiquitous and reporting is mandated but &lt;a href=&#34;https://nces.ed.gov/blogs/nces/post/free-or-reduced-price-lunch-a-proxy-for-poverty&#34;&gt;there is debate as to whether it actually reflects the level of poverty among students&lt;/a&gt;.)&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/blog/why-disaggregate-data_files/figure-html/unnamed-chunk-3-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;When we look at these data, MPS looks like a diverse school district. Almost &lt;strong&gt;40% of students are Black&lt;/strong&gt; and around &lt;strong&gt;35% are White&lt;/strong&gt;. &lt;strong&gt;60% of the students are eligible for FRPL&lt;/strong&gt;, which is &lt;a href=&#34;https://nces.ed.gov/programs/digest/d17/tables/dt17_204.10.asp?current=yes&#34;&gt;high for Minnesota but close to the US average of 52.1%&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;However, let’s explore if there’s more to this story.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;discover-distributions&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Discover Distributions&lt;/h3&gt;
&lt;p&gt;Another view of the data can be visualizing the distribution of percentage of a demographic within schools. Here is a histogram for the percentage of White students within the 74 MPS schools for which we have data.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/blog/why-disaggregate-data_files/figure-html/unnamed-chunk-4-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;27 of the 74 (36%) of schools have between 0-10% White students.&lt;/strong&gt; This implies that even though the school district may be diverse, the demographics are not evenly distributed across the schools. More than half of schools enroll fewer than 30% of White students even though White students make up 35% of the district student population.&lt;/p&gt;
&lt;p&gt;The school race demographics are not representative of the district populations but does that hold for socioeconomic status as well?&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;create-categories&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Create Categories&lt;/h3&gt;
&lt;p&gt;High-poverty schools are defined as public schools where more than 75% of the students are eligible for FRPL. According to NCES, &lt;a href=&#34;https://nces.ed.gov/fastfacts/display.asp?id=898&#34;&gt;24% of public school students attended high-poverty schools&lt;/a&gt;. However, different subgroups were overrepresented and underrepresented within the high poverty schools. Is this the case for MPS?&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/blog/why-disaggregate-data_files/figure-html/unnamed-chunk-6-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;9% of White students&lt;/strong&gt; attend high poverty schools, compared to &lt;strong&gt;46% of Black students&lt;/strong&gt;, &lt;strong&gt;51% of Hispanic students&lt;/strong&gt;, &lt;strong&gt;46% of Asian students&lt;/strong&gt;, and &lt;strong&gt;49% of Native American students&lt;/strong&gt;. These students are disproportionally attending high poverty schools.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;reveal-relationships&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Reveal Relationships&lt;/h3&gt;
&lt;p&gt;Let’s explore what happens when we correlate race and FRPL percentage by school.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/blog/why-disaggregate-data_files/figure-html/unnamed-chunk-7-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Similarly to the result above, &lt;strong&gt;there is a strong negative correlation between FRPL percentage and the percentage of white students in a school.&lt;/strong&gt; High poverty schools have a lower percentage of White students and low poverty schools have a higher percentage of White students.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;conclusion&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Conclusion&lt;/h3&gt;
&lt;p&gt;According to the Urban Institute, the disproportionate percentage of students of color attending high poverty schools “is a defining feature of almost all Midwestern and northeastern metropolitan school systems.” Among other issues, high poverty schools &lt;a href=&#34;https://www.urban.org/urban-wire/high-poverty-schools-undermine-education-children-color&#34;&gt;tend to lack the educational resources—like highly qualified and experienced teachers, low student-teacher ratios, college prerequisite and advanced placement courses, and extracurricular activities—available in low-poverty schools.&lt;/a&gt; This has a huge impact on these students and their futures.&lt;/p&gt;
&lt;p&gt;Because of the disaggregated data Minneapolis Public Schools provides, we can go deeper than the average of demographics across the district and see what it looks like on the school level. These views display that (1) there exists a distribution of race/ethnicity within schools that are not representative of the district, (2) that students of color are overrepresented in high poverty schools, and (3) there is a relationship between the percentage of White students in a school and the percentage of students eligible for FRPL.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;amazing-examples-of-disaggregated-data-reporting&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Amazing Examples of Disaggregated Data Reporting&lt;/h2&gt;
&lt;p&gt;There are so many amazing examples out there using disaggregated data. These two are my favorites:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://www.nytimes.com/interactive/2016/04/29/upshot/money-race-and-success-how-your-school-district-compares.html&#34;&gt;Money, Race and Success: How Your School District Compares&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://www.tampabay.com/projects/2015/investigations/pinellas-failure-factories/chart-failing-black-students/&#34;&gt;Why Pinellas County is the worst place in Florida to be black and go to public school&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;code-appendix&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Code Appendix&lt;/h2&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(tidyverse)
library(viridis)
library(tabulizer)
library(janitor)

# race data
race_pdf &amp;lt;-
  extract_tables(&amp;quot;http://studentaccounting.mpls.k12.mn.us/uploads/racial_ethnic_school_gradefall2017.pdf&amp;quot;)

race_df &amp;lt;- # many thanks to my brother @gvelasq for purrrifying this
  race_pdf %&amp;gt;%
  map(as_tibble) %&amp;gt;%
  map_df(~ slice(., -1:-2)) %&amp;gt;% 
  set_names(c(&amp;quot;school_group&amp;quot;, &amp;quot;school_name&amp;quot;, &amp;quot;grade&amp;quot;, &amp;quot;na_num&amp;quot;, &amp;quot;na_pct&amp;quot;, &amp;quot;aa_num&amp;quot;, &amp;quot;aa_pct&amp;quot;, &amp;quot;as_num&amp;quot;, &amp;quot;as_pct&amp;quot;, &amp;quot;hi_num&amp;quot;, &amp;quot;hi_pct&amp;quot;, &amp;quot;wh_num&amp;quot;, &amp;quot;wh_pct&amp;quot;, &amp;quot;pi_pct&amp;quot;, &amp;quot;blank_col&amp;quot;, &amp;quot;tot&amp;quot;))

race_filter &amp;lt;-
  race_df %&amp;gt;%
  select(-school_group, -grade, -pi_pct, -blank_col) %&amp;gt;% # unnecessary or blank columns
  filter(str_detect(school_name, &amp;quot;Total&amp;quot;),
         school_name != &amp;quot;Grand Total&amp;quot;) %&amp;gt;% # otherwise totals are duplicated
  mutate(school_name = str_replace(school_name, &amp;quot;Total&amp;quot;, &amp;quot;&amp;quot;)) %&amp;gt;% 
  mutate_if(is.character, trimws) 

# frpl data
frpl_pdf &amp;lt;-
  extract_tables(&amp;quot;http://studentaccounting.mpls.k12.mn.us/uploads/free_reduced_meal_fall_2017.pdf&amp;quot;)

frpl_df &amp;lt;- # many thanks to my brother @gvelasq for purrrifying this
  frpl_pdf %&amp;gt;%
  map(as_tibble) %&amp;gt;%
  map_df(~ slice(., -1)) %&amp;gt;%  
  set_names(c(&amp;quot;school_grades&amp;quot;, &amp;quot;school_name&amp;quot;, &amp;quot;total_students&amp;quot;, &amp;quot;frpl_pct&amp;quot;, &amp;quot;free_num&amp;quot;, &amp;quot;reduce_num&amp;quot;, &amp;quot;not_eligible_num&amp;quot;))

frpl_filter &amp;lt;-
  frpl_df %&amp;gt;% 
  filter(school_name != &amp;quot;&amp;quot;) %&amp;gt;%
  select(-school_grades)

# merged data
merged_df &amp;lt;-
  left_join(race_filter, frpl_filter, by = c(&amp;quot;school_name&amp;quot;)) %&amp;gt;% 
  mutate_at(2:17, as.numeric) %&amp;gt;%
  mutate(frpl_pct = (free_num + reduce_num)/total_students,
         hi_povnum = case_when(frpl_pct &amp;gt; .75 ~ hi_num),
         aa_povnum = case_when(frpl_pct &amp;gt; .75 ~ aa_num),
         wh_povnum = case_when(frpl_pct &amp;gt; .75 ~ wh_num),
         as_povnum = case_when(frpl_pct &amp;gt; .75 ~ as_num),
         na_povnum = case_when(frpl_pct &amp;gt; .75 ~ na_num)) %&amp;gt;%
  adorn_totals() %&amp;gt;%
  mutate(na_pct = na_num/tot,
         aa_pct = aa_num/tot,
         as_pct = as_num/tot,
         hi_pct = hi_num/tot,
         wh_pct = wh_num/tot,
         frpl_pct = (free_num + reduce_num)/total_students, # otherwise total frpl_pct is off
         hi_povsch = hi_povnum/hi_num[which(school_name == &amp;quot;Total&amp;quot;)],
         aa_povsch = aa_povnum/aa_num[which(school_name == &amp;quot;Total&amp;quot;)],
         as_povsch = as_povnum/as_num[which(school_name == &amp;quot;Total&amp;quot;)],
         wh_povsch = wh_povnum/wh_num[which(school_name == &amp;quot;Total&amp;quot;)],
         na_povsch = na_povnum/na_num[which(school_name == &amp;quot;Total&amp;quot;)])

# tidy data
tidy_df &amp;lt;-
  merged_df %&amp;gt;%
  gather(category, value, -school_name)

# this is so I do not have to run the code above constantly
tidy_df &amp;lt;- read_csv(here::here(&amp;quot;content&amp;quot;, &amp;quot;blog&amp;quot;, &amp;quot;why_disaggregate_data_files&amp;quot;, &amp;quot;tidy_df.csv&amp;quot;))
merged_df &amp;lt;- read_csv(here::here(&amp;quot;content&amp;quot;, &amp;quot;blog&amp;quot;, &amp;quot;why_disaggregate_data_files&amp;quot;, &amp;quot;merged_df.csv&amp;quot;))
# demographic barplot
tidy_df %&amp;gt;%
  filter(school_name == &amp;quot;Total&amp;quot;,
         str_detect(category, &amp;quot;pct&amp;quot;)) %&amp;gt;% 
  mutate(category = factor(category, levels = c(&amp;quot;aa_pct&amp;quot;, &amp;quot;wh_pct&amp;quot;, &amp;quot;hi_pct&amp;quot;, &amp;quot;as_pct&amp;quot;, &amp;quot;na_pct&amp;quot;, &amp;quot;frpl_pct&amp;quot;))) %&amp;gt;%  
  ggplot(aes(x = category, y = value)) +
  geom_bar(stat = &amp;quot;identity&amp;quot;, aes(fill = factor(category))) +
  xlab(&amp;quot;Subgroup&amp;quot;) +
  ylab(&amp;quot;Percentage of Population&amp;quot;) +
  scale_x_discrete(labels = c(&amp;quot;Black&amp;quot;, &amp;quot;White&amp;quot;, &amp;quot;Hispanic&amp;quot;, &amp;quot;Asian&amp;quot;, &amp;quot;Native Am.&amp;quot;, &amp;quot;FRPL&amp;quot;)) +
  scale_y_continuous(labels = scales::percent) +
  scale_fill_viridis(option = &amp;quot;cividis&amp;quot;, discrete = T) +
  theme_minimal() +
  theme(legend.position = &amp;quot;none&amp;quot;)

# histogram
merged_df %&amp;gt;% 
  filter(school_name != &amp;quot;Total&amp;quot;) %&amp;gt;% 
  ggplot(aes(x = wh_pct)) +
  geom_histogram(fill = &amp;quot;slategray4&amp;quot;, breaks= seq(0, 1, by = .1)) +
  xlab(&amp;quot;White Percentage&amp;quot;) +
  ylab(&amp;quot;Count&amp;quot;) +
  scale_x_continuous(labels = scales::percent) +
  theme_minimal() +
  theme(legend.position = &amp;quot;none&amp;quot;)

# histogram info
histinfo &amp;lt;- hist(merged_df$wh_pct)
histinfo

# high poverty barplot
tidy_df %&amp;gt;%
  filter(school_name == &amp;quot;Total&amp;quot;,
         str_detect(category, &amp;quot;povsch&amp;quot;)) %&amp;gt;% 
  mutate(category = factor(category, levels = c(&amp;quot;hi_povsch&amp;quot;, &amp;quot;na_povsch&amp;quot;, &amp;quot;aa_povsch&amp;quot;, &amp;quot;as_povsch&amp;quot;, &amp;quot;wh_povsch&amp;quot;))) %&amp;gt;%  
  ggplot(aes(x = category, y = value)) +
  geom_bar(stat = &amp;quot;identity&amp;quot;, aes(fill = factor(category))) +
  xlab(&amp;quot;Subgroup&amp;quot;) +
  ylab(&amp;quot;Percentage in High Poverty Schools&amp;quot;) +
  scale_x_discrete(labels = c(&amp;quot;Hispanic&amp;quot;, &amp;quot;Native Am.&amp;quot;, &amp;quot;Black&amp;quot;, &amp;quot;Asian&amp;quot;, &amp;quot;White&amp;quot;)) +
  scale_y_continuous(labels = scales::percent) +
  scale_fill_viridis(option = &amp;quot;cividis&amp;quot;, discrete = T) +
  theme_minimal() +
  theme(legend.position = &amp;quot;none&amp;quot;)

# scatterplot
merged_df %&amp;gt;% 
  filter(school_name != &amp;quot;Total&amp;quot;) %&amp;gt;% 
  ggplot(aes(x = wh_pct, y = frpl_pct)) +
  geom_point(color = &amp;quot;slategray4&amp;quot;) +
  xlab(&amp;quot;White Percentage&amp;quot;) +
  ylab(&amp;quot;FRPL Percentage&amp;quot;) +
  scale_y_continuous(labels = scales::percent) +
  scale_x_continuous(labels = scales::percent) +
  theme_minimal() +
  theme(legend.position = &amp;quot;none&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
</description>
  </item>
  
</channel>
  </rss>